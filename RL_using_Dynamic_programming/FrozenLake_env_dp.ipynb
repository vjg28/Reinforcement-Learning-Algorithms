{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import time\n",
    "#make a python file of dynamic_programming_functions as dp.py to import policy-value iteration\n",
    "from dp import policy_iteration, value_iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_mappings = {\n",
    "    0: '\\u2191', #up\n",
    "    1: '\\u2192', #right \n",
    "    2: '\\u2193', #down\n",
    "    3: '\\u2190', #left\n",
    "}\n",
    "\n",
    "#print(''.join([action_mappings[action] for action in range(4)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove the comment portion of the code only if one needs to see the render process \n",
    "#and the environment\n",
    "n_episodes=10000\n",
    "\n",
    "def play_episodes(environment, n_episodes, policy):\n",
    "    wins =0\n",
    "    total_reward=0\n",
    "#    l=0\n",
    "#    k=0\n",
    "    for episode in range(n_episodes):\n",
    "        terminated=False\n",
    "        state=environment.reset()\n",
    "        \n",
    "        while not terminated:\n",
    "            \n",
    "            action=np.argmax(policy[state])\n",
    "            next_state, reward, terminated, info = environment.step(action)#\n",
    "\n",
    "#            if l<=10:\n",
    "#                print(k)\n",
    "#                environment.render()\n",
    "#                k+=1\n",
    "\n",
    "            total_reward+=reward\n",
    "            state=next_state\n",
    "            if terminated and reward ==1.0:\n",
    "                wins +=1\n",
    "\n",
    "#                print('win')\n",
    "#        l=l+1\n",
    "    \n",
    "    average_reward=total_reward/n_episodes\n",
    "    \n",
    "    return wins, total_reward, average_reward\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "solvers=[('Policy Iteration', policy_iteration),('Value Iteration', value_iteration)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment Created.\n",
      "Running Policy Iteration .....\n",
      "Policy evaluated in 203 iteration\n",
      "Policy evaluated in 862 iteration\n",
      "Policy evaluated in 907 iteration\n",
      "Policy evaluated in 958 iteration\n",
      "Policy evaluated in 1065 iteration\n",
      "Policy evaluated in 936 iteration\n",
      "Evaluated 7 policies.\n",
      "Done.\n",
      "\n",
      " Final Policy derived using Policy Iteration:\n",
      "→↓↓↓↓↓↓↓←←←←←←←↓↑↑↑↑↓←←↓↑↑↑→↑↑↓↓↑←↑↑↓→←↓↑↑↑→←↑↑↓↑↑→↑↑↑↑↓↑→↑↑→↓→↑\n",
      "1.920724 seconds taken by Policy Iteration\n",
      "Policy Iteration :: number of wins over 10000 episodes = 8830\n",
      "Policy Iteration :: average reward over 10000 episodes = 0.88 \n",
      "\n",
      "Environment Created.\n",
      "Running Value Iteration .....\n",
      "Value iteration converged at iteration #914\n",
      "Done.\n",
      "\n",
      " Final Policy derived using Value Iteration:\n",
      "↓↓↓↓↓↓↓↓←←←←←←←↓↑↑↑↑↓←←↓↑↑↑→↑↑↓↓↑←↑↑↓→←↓↑↑↑→←↑↑↓↑↑→↑↑↑↑↓↑→↑↑→↓→↑\n",
      "0.642869 seconds taken by Value Iteration\n",
      "Value Iteration :: number of wins over 10000 episodes = 8827\n",
      "Value Iteration :: average reward over 10000 episodes = 0.88 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for iteration_name, iteration_func in solvers:\n",
    "     \n",
    "        environment=gym.make('FrozenLake8x8-v0')\n",
    "        print('Environment Created.')\n",
    "        start=time.time()\n",
    "#Print Start and end if one needs to get familiar with time library.\n",
    "#        print(start)                \n",
    "        print('Running %s .....' %iteration_name)\n",
    "        policy, V =iteration_func(environment.env)\n",
    "        print('Done.')\n",
    "        end=time.time()\n",
    "#        print(end)\n",
    "        \n",
    "        print('\\n Final Policy derived using %s:' %iteration_name)\n",
    "        print(''.join([action_mappings[action] for action in np.argmax(policy, axis=1)]))\n",
    "#        print(policy)\n",
    "        print('%f seconds taken by %s' %(end-start, iteration_name))\n",
    "        wins, total_reward, average_reward = play_episodes(environment, n_episodes, policy)\n",
    "        \n",
    "        print('%s :: number of wins over %d episodes = %d' %(iteration_name,n_episodes, wins))\n",
    "        print('%s :: average reward over %d episodes = %.2f \\n' %(iteration_name, n_episodes, average_reward))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Some links that may help:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.youtube.com/watch?v=5R2vErZn0yw\n",
    "\n",
    "Navigating a Virtual World Using Dynamic Programming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/search?utf8=%E2%9C%93&q=dynamic+programming+RL&type=Commits\n",
    "\n",
    "Links to RL dynamic programming cases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/josjo80/deep-learning/commit/cdf5d5afc4efb4f4d4c4c0f7d3615cceff1593ef\n",
    "    \n",
    "Guided links to some codes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/karim-berrada/Reinforcement_Learning_projects/commit/5746c3b8b2ea61ff905bcb60b6c22bd423121de6\n",
    "\n",
    "To solve some interesting problems based on Dynamic programming.\"One site cutting tree problem\" using dp."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
