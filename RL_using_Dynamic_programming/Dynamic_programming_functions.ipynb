{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def one_step_lookahead(environment, state, V, discount):\n",
    "    \"\"\"\n",
    "    helper function to calculate the value function\n",
    "    \n",
    "    \"\"\"\n",
    "    #Creating a vector of dimensionally same size as the number of actions\n",
    "    action_values=np.zeros(environment.nA)\n",
    "    \n",
    "    for action in range(environment.nA):\n",
    "        \n",
    "        for probability, next_state, reward, terminated in environment.P[state][action]: #policy\n",
    "            action_values[action] +=  probability * (reward + discount_factor * V[next_state])\n",
    "            \n",
    "    return action_values    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_evaluation(policy, environment, discount_factor=1.0, theta=1e-9, max_iteration=1e9):\n",
    "    \"\"\"\n",
    "    evaluate a policy given a deterministic environment\n",
    "    \n",
    "    1)policy : Matrix of size nS*nA. Each cell reprents the probability of \n",
    "               taking an action in a particular state\n",
    "    2)Environment : openAI environment object\n",
    "    3)discount_factor:\n",
    "    4)theta: Convergence factor. If the change in value function for all \n",
    "             states is below theta, we are done.\n",
    "    5)max_iterations: To avoid infinite looping.\n",
    "    \n",
    "    Returns:\n",
    "    1)V:The optimum value estimate for the given policy \n",
    "    \"\"\"\n",
    "    \n",
    "    evaluation_iterations = 1 # to record the number of iterations\n",
    "    V=np.zeros(environment.nS)\n",
    "    \n",
    "    for i in range(int(max_iterations)):\n",
    "        delta = 0  #for early stopping\n",
    "        \n",
    "        for state in range(environment.nS):\n",
    "            v=0\n",
    "            \n",
    "            for action, action_probability in enumerate(policy[state]):\n",
    "                \n",
    "                for state_probability, next_state, reward, terminated in environment.P[state][action]:\n",
    "                    v+= action_probability * state_probability * (reward + discount_factor *V[next_state])\n",
    "        \n",
    "            delta= max(delta, abs(V[state]-v)) #marked\n",
    "            V[state]=v\n",
    "\n",
    "        evaluation_iterations +=1\n",
    "        \n",
    "        if(delta < theta):\n",
    "            print('policy evaluated in d% iterations' % evaluation_iterations)\n",
    "            return V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_iteration(environment, discount_factor=1.0, max_iterations=1e9):\n",
    "    \n",
    "    \"\"\"\n",
    "    In this function, we would take a random policy and evaluate the optimum \n",
    "    value function of the policy ,act greedily on the policy and work for the\n",
    "    new better policy.\n",
    "    \"\"\"\n",
    "    \n",
    "    policy=np.ones((environment.nS, environment.nA))/environment.nA\n",
    "    \n",
    "    evaulated_policies =1\n",
    "    \n",
    "    for i in range(int(max_iterations)):\n",
    "        \n",
    "        stable_policy= True\n",
    "        V=policy_evaluation(policy,environment, discount_factor=discount_factor)\n",
    "        \n",
    "        for state in range(environment.nS):\n",
    "            \n",
    "            current_action=np.argmax(policy[state])    #error here what if elements are same?\n",
    "            action_values=one_step_lookahead(environment, state,V ,discount_factor=discount_factor)\n",
    "            best_action = np.argmax(action_values)\n",
    "            \n",
    "            if(current_action != best_action):\n",
    "                stable_policy =False\n",
    "                \n",
    "            policy[state]=np.eye(environment.nA)[best_action]\n",
    "            \n",
    "        evaluated_policies +=1\n",
    "        \n",
    "        if(stable_policy):\n",
    "            print('Evaluated %d policies.' % evaluated_policies)\n",
    "            return policy, V\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_iteration(environment, discount_factor=1.0, theta=1e-9, max_iterations=1e9):\n",
    "    \n",
    "    V=np.zeros(environment.nS)\n",
    "    \n",
    "    for i in range(int(max_iterations)):\n",
    "        \n",
    "        delta=0\n",
    "        \n",
    "        for state in range(environment.nS):\n",
    "            \n",
    "            action_values=one_step_lookahead(environment, state, V, discount_factor)\n",
    "            best_action_value=np.max(action_values)\n",
    "            delta=max(delta,abs(V[state]-best_action_value))\n",
    "            V[state]=best_action_value\n",
    "            \n",
    "        if(delta <theta):\n",
    "            print('Value iteration converged at iteration #%d' % i)\n",
    "            break\n",
    "    \n",
    "    policy= np.zeros((environment.nS, environment.nA))\n",
    "    \n",
    "    for state in  range(environment.nS):\n",
    "        \n",
    "        action_values= one_step_lookahead(environment, state, V, discount_factor)\n",
    "        best_action = np.argmax(action_values)\n",
    "        policy[state][best_action]=1.0\n",
    "        \n",
    "    return policy, V\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### PythonLearning1: \n",
    "The arrays in numpy are somewhat different than those found in real manipulations.\n",
    "    for eg: \n",
    "    1)\n",
    "        import numpy as np\n",
    "        a=np.array([1,2,3])\n",
    "        print(a)    # printing A\n",
    "        print(a.T)  #printing A transpose\n",
    "        \n",
    "    Output:\n",
    "        array([1,2,3])\n",
    "        array([1,2,3])\n",
    "    \n",
    "    2)\n",
    "        import numpy as np\n",
    "        a=np.array([1,2,3])[np.newaxis]\n",
    "        print(a)    # printing A\n",
    "        print(a.T)  #printing A transpose\n",
    "\n",
    "    Output:\n",
    "        array([1,2,3])\n",
    "        array([[1],[2],[3]])\n",
    " There is no direct way in python language to deal with arrays and transpose of arrays. Therefore we use numpy.\n",
    "To get more info about the transpose thing, follow this link.\n",
    "\n",
    "https://stackoverflow.com/questions/5954603/transposing-a-numpy-array\n",
    "    \n",
    "   \"\"\"           "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PythonLearning2:\n",
    "The output of the code in python:\n",
    "      for action in range(10):\n",
    "...     print(action)\n",
    "   Output:\n",
    "        0\n",
    "        1\n",
    "        2\n",
    "        3\n",
    "        4\n",
    "        5\n",
    "        6\n",
    "        7\n",
    "        8\n",
    "        9\n",
    " which means the range function output starts from 0 and end at 9 < 10."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PythonLearning3 : \n",
    "Enumerate function:\n",
    "\n",
    "    seasons = ['Spring', 'Summer', 'Fall', 'Winter']\n",
    "    list(enumerate(seasons))\n",
    "    [(0, 'Spring'), (1, 'Summer'), (2, 'Fall'), (3, 'Winter')]\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
